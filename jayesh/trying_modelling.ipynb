{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "result = os.environ['PATH']\n",
    "os.environ['PATH'] += os.pathsep + 'C:\\\\Program Files\\\\ffmpeg-2024-09-22-git-a577d313b2-full_build\\\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_audio = AudioSegment.from_file(\"final_audio_with_drinking_events.wav\")\n",
    "event_df = pd.read_csv(\"drinking_event_times.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = []\n",
    "\n",
    "# for _, row in event_df.iterrows():\n",
    "#     for onefourth_minute in range(row['start_onefourth_minute'], row['end_onefourth_minute'] + 1):\n",
    "#         rows.append({\"onefourth_minute\": onefourth_minute, \"water_drinking\": 1})\n",
    "\n",
    "# # Create a DataFrame from rows and remove duplicates\n",
    "# drinking_df = pd.DataFrame(rows).drop_duplicates()\n",
    "\n",
    "# # Fill in missing quarter-minute intervals with water_drinking = 0\n",
    "# all_onefourth_minutes = range(drinking_df['onefourth_minute'].min(), drinking_df['onefourth_minute'].max() + 1)\n",
    "# all_onefourth_minutes_df = pd.DataFrame({\"onefourth_minute\": all_onefourth_minutes})\n",
    "# final_df = all_onefourth_minutes_df.merge(drinking_df, on=\"onefourth_minute\", how=\"left\")\n",
    "# final_df['water_drinking'] = final_df['water_drinking'].fillna(0).astype(int)\n",
    "\n",
    "# def split_and_label_audio(audio, final_df, sr=16000, window_size=30, overlap=15):\n",
    "#     labeled_segments = []\n",
    "#     step = window_size - overlap  # Compute step size based on overlap\n",
    "    \n",
    "#     for idx, row in final_df.iterrows():\n",
    "#         # Convert 'onefourth_minute' to milliseconds\n",
    "#         start_time = row['onefourth_minute'] * 15 * 1000  # Convert quarter minute to milliseconds\n",
    "#         end_time = start_time + window_size * 1000         # 30 seconds later\n",
    "        \n",
    "#         # Check if the drinking event is in the current row or the next row\n",
    "#         if row['water_drinking'] == 1:\n",
    "#             label = 1\n",
    "#         elif idx + 1 < len(final_df) and final_df.iloc[idx + 1]['water_drinking'] == 1:\n",
    "#             label = 1\n",
    "#         else:\n",
    "#             label = 0\n",
    "        \n",
    "#         # Extract the audio segment based on calculated start and end times\n",
    "#         segment = audio[start_time:end_time]\n",
    "        \n",
    "#         # Append the segment and its label\n",
    "#         labeled_segments.append((segment, label))\n",
    "        \n",
    "#         # Move to the next window with overlap\n",
    "#         start_time += step * 1000  # Move by step size in milliseconds\n",
    "#         end_time = start_time + window_size * 1000  # Update end time\n",
    "\n",
    "#     return labeled_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(audio_segment, sr=16000, n_mfcc=13):\n",
    "    samples = np.array(audio_segment.get_array_of_samples()).astype(np.float32) / 32768.0\n",
    "    mfcc = librosa.feature.mfcc(y=samples, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfcc.T  # Transpose to (time, n_mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sample_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1041</td>\n",
       "      <td>10016</td>\n",
       "      <td>library_water_bottle_0_elvis_162_aug2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196518</td>\n",
       "      <td>206521</td>\n",
       "      <td>home_water_bottle_0_elvis_111_aug1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408802</td>\n",
       "      <td>419868</td>\n",
       "      <td>library_water_bottle_0_elvis_152_aug1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>439012</td>\n",
       "      <td>450013</td>\n",
       "      <td>library_water_bottle_0_elvis_206_aug1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>535584</td>\n",
       "      <td>546568</td>\n",
       "      <td>home_water_bottle_0_elvis_179_aug2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>34464952</td>\n",
       "      <td>34475896</td>\n",
       "      <td>home_water_bottle_0_elvis_127_aug2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>34513573</td>\n",
       "      <td>34521615</td>\n",
       "      <td>library_water_bottle_0_elvis_201_aug1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>34728409</td>\n",
       "      <td>34738423</td>\n",
       "      <td>home_water_bottle_0_elvis_119_aug2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>34803992</td>\n",
       "      <td>34815992</td>\n",
       "      <td>library_water_bottle_0_elvis_155_aug2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>34976487</td>\n",
       "      <td>34988558</td>\n",
       "      <td>home_water_bottle_0_elvis_114_aug0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        start       end                            sample_name\n",
       "0        1041     10016  library_water_bottle_0_elvis_162_aug2\n",
       "1      196518    206521     home_water_bottle_0_elvis_111_aug1\n",
       "2      408802    419868  library_water_bottle_0_elvis_152_aug1\n",
       "3      439012    450013  library_water_bottle_0_elvis_206_aug1\n",
       "4      535584    546568     home_water_bottle_0_elvis_179_aug2\n",
       "..        ...       ...                                    ...\n",
       "325  34464952  34475896     home_water_bottle_0_elvis_127_aug2\n",
       "326  34513573  34521615  library_water_bottle_0_elvis_201_aug1\n",
       "327  34728409  34738423     home_water_bottle_0_elvis_119_aug2\n",
       "328  34803992  34815992  library_water_bottle_0_elvis_155_aug2\n",
       "329  34976487  34988558     home_water_bottle_0_elvis_114_aug0\n",
       "\n",
       "[330 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>start_seconds</th>\n",
       "      <th>end_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1041</td>\n",
       "      <td>10016</td>\n",
       "      <td>library_water_bottle_0_elvis_162_aug2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end                            sample_name  start_seconds  \\\n",
       "0   1041  10016  library_water_bottle_0_elvis_162_aug2              1   \n",
       "\n",
       "   end_seconds  \n",
       "0           10  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 0\n",
    "end = 30\n",
    "\n",
    "event_df[\n",
    "    ((event_df['start_seconds'] >= start) & (event_df['start_seconds'] <= end)) |\n",
    "    ((event_df['end_seconds'] >= start) & (event_df['end_seconds'] <= end))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_df start and end seconds\n",
    "event_df['start_seconds'] = event_df['start'] // 1000\n",
    "event_df['end_seconds'] = event_df['end'] // 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column called duration in seconds\n",
    "event_df['duration'] = event_df['end_seconds'] - event_df['start_seconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3228"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get sum \n",
    "event_df['duration'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio, event_df, sr=16000, window_size=30, overlap=15):\n",
    "    segments = []\n",
    "    step = window_size - overlap  # Compute step size based on overlap\n",
    "    \n",
    "    start_time = 0\n",
    "    end_time = window_size * 1000  # Convert window size to milliseconds\n",
    "    \n",
    "    while end_time <= len(audio):\n",
    "        # Extract the audio segment based on calculated start and end times\n",
    "        segment = audio[start_time:end_time]\n",
    "        \n",
    "        # Check if the segment contains any event\n",
    "        start_seconds = start_time // 1000\n",
    "        end_seconds = end_time // 1000\n",
    "        event_df[\n",
    "            ((event_df['start_seconds'] >= start_seconds) & (event_df['start_seconds'] <= end_seconds)) |\n",
    "            ((event_df['end_seconds'] >= start_seconds) & (event_df['end_seconds'] <= end_seconds))\n",
    "        ]\n",
    "        \n",
    "        if len(event_df) > 0:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        # Append the segment\n",
    "        segments.append(segment)\n",
    "        \n",
    "        # Move to the next window with overlap\n",
    "        start_time += step * 1000  # Move by step size in milliseconds\n",
    "        end_time = start_time + window_size * 1000  # Update end time\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the audio into 30 second segments\n",
    "audio_segments = split_audio(final_audio, event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAudioClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepAudioClassifierCNN, self).__init__()\n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout2d(0.3)  # Regularization\n",
    "        \n",
    "        # Second Convolutional Block with Residual Connection\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv1x1_1 = nn.Conv2d(32, 64, kernel_size=1)  # Match residual channels\n",
    "        self.dropout2 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        # Third Convolutional Block with Residual Connection\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        self.conv1x1_2 = nn.Conv2d(64, 128, kernel_size=1)  # Match residual channels\n",
    "        self.dropout3 = nn.Dropout2d(0.3)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self._get_conv_output_size(), 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)  # Binary classification\n",
    "\n",
    "    def _get_conv_output_size(self):\n",
    "        # Create a dummy input to calculate the size after convolution layers\n",
    "        dummy_input = torch.zeros(1, 1, 1876, 13)  # Adjust according to your MFCC input\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(dummy_input))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Apply until last conv layer\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "        return int(torch.prod(torch.tensor(x.size()[1:])))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second Convolutional Block with Residual Connection\n",
    "        residual = self.conv1x1_1(x)  # Adjust residual channels to match conv output\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x + residual  # Replace in-place addition\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Third Convolutional Block with Residual Connection\n",
    "        residual = self.conv1x1_2(x)  # Adjust residual channels to match conv output\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = x + residual  # Replace in-place addition\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Flatten for Fully Connected Layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation for output, use CrossEntropyLoss\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create the model instance\n",
    "model = DeepAudioClassifierCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepAudioClassifierCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1x1_1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1x1_2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (dropout3): Dropout2d(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=29952, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pytorch model\n",
    "path = \"audio_classifier_cnn.pth\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2339/2339 [03:39<00:00, 10.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# pass the audio segments through the model\n",
    "predictions = []\n",
    "for segment in tqdm(audio_segments):\n",
    "    mfcc = extract_mfcc(segment)\n",
    "    mfcc = torch.tensor(mfcc).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    output = model(mfcc)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predictions.append(predicted.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df of every second of the audio\n",
    "all_seconds = len(final_audio) // 1000\n",
    "all_seconds_df = pd.DataFrame({\"second\": range(all_seconds)})\n",
    "all_seconds_df['prediction'] = 0\n",
    "\n",
    "# fill in the predictions\n",
    "for idx, row in event_df.iterrows():\n",
    "    all_seconds_df.loc[\n",
    "        (all_seconds_df['second'] >= row['start_seconds']) & (all_seconds_df['second'] <= row['end_seconds']),\n",
    "        'prediction'\n",
    "    ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the audios with their starting point as key where the prediction is 1\n",
    "audio_segments_with_predictions = {}\n",
    "for i, audio in enumerate(audio_segments):\n",
    "    if predictions[i] == 1:\n",
    "        start = i * 15\n",
    "        audio_segments_with_predictions[start] = audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_segments_with_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split every 30 seconds of the audio into 1 second segments\n",
    "one_second_segments = {}\n",
    "for start, audio in audio_segments_with_predictions.items():\n",
    "    for i in range(0, len(audio), 1000):\n",
    "        segment = audio[i:i+1000]\n",
    "        time = start + i // 1000\n",
    "        one_second_segments[time] = segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15645"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_second_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows which are not in the one_second_segments\n",
    "all_seconds_df['keep'] = False\n",
    "for audio_start in audio_segments_with_predictions.keys():\n",
    "    for i in range(0, 30):\n",
    "        all_seconds_df.loc[all_seconds_df['second'] == audio_start + i, 'keep'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seconds_df = all_seconds_df[all_seconds_df['keep']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "all_seconds_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15645 entries, 0 to 35009\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   second      15645 non-null  int64\n",
      " 1   prediction  15645 non-null  int64\n",
      " 2   keep        15645 non-null  bool \n",
      "dtypes: bool(1), int64(2)\n",
      "memory usage: 382.0 KB\n"
     ]
    }
   ],
   "source": [
    "all_seconds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       "0    12113\n",
       "1     3532\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seconds_df['prediction'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>second</th>\n",
       "      <th>prediction</th>\n",
       "      <th>keep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   second  prediction  keep\n",
       "0       0           0  True\n",
       "1       1           1  True\n",
       "2       2           1  True\n",
       "3       3           1  True\n",
       "4       4           1  True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seconds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_second_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_label_audio(audio_dict, event_df, sr=16000, window_size=2, overlap=1):\n",
    "    \"\"\"\n",
    "    Create 3-second audio segments with 1-second overlap, labeling based on drinking events.\n",
    "    \n",
    "    Args:\n",
    "        audio_dict (dict): Dictionary mapping seconds to audio segments.\n",
    "        event_df (DataFrame): DataFrame with 'second' and 'prediction' columns.\n",
    "        sr (int): Sample rate (default is 16000).\n",
    "        window_size (int): Length of each window in seconds.\n",
    "        overlap (int): Overlap between consecutive windows in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples (AudioSegment, label).\n",
    "    \"\"\"\n",
    "    labeled_segments = []\n",
    "    step = window_size - overlap  # Compute step size for overlapping windows\n",
    "    \n",
    "    # Determine the range of seconds available in the audio dictionary\n",
    "    audio_seconds = sorted(audio_dict.keys())\n",
    "    min_second = min(audio_seconds)\n",
    "    max_second = max(audio_seconds)\n",
    "    \n",
    "    # Iterate over the start times for the windows\n",
    "    start_second = min_second\n",
    "    while start_second + window_size <= max_second + 1:\n",
    "        # Collect the audio for the current 3-second window\n",
    "        segment_audio = []\n",
    "        for second in range(start_second, start_second + window_size):\n",
    "            if second in audio_dict:  # Check if the second has audio\n",
    "                segment_audio.append(audio_dict[second])\n",
    "        \n",
    "        # Combine the audio segments into a single segment if audio exists\n",
    "        if segment_audio:\n",
    "            combined_audio = sum(segment_audio)  # Combine pydub AudioSegments\n",
    "            \n",
    "            # Determine the label for the window\n",
    "            label = event_df[\n",
    "                (event_df['second'] >= start_second) & (event_df['second'] < start_second + window_size)\n",
    "            ]['prediction'].max()  # Label is 1 if any second in the window has prediction = 1\n",
    "            \n",
    "            labeled_segments.append((combined_audio, label))\n",
    "        \n",
    "        # Move the window forward\n",
    "        start_second += step\n",
    "    \n",
    "    return labeled_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segments = split_and_label_audio(audio_dict=one_second_segments, event_df=all_seconds_df)\n",
    "\n",
    "# all_mfccs = []\n",
    "# all_labels = []\n",
    "\n",
    "# for segment, label in tqdm(audio_segments):\n",
    "#     mfcc = extract_mfcc(segment)\n",
    "#     all_mfccs.append(mfcc)\n",
    "#     all_labels.append(label)\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(all_mfccs, all_labels, test_size=0.25, random_state=42, stratify=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_mels=64,\n",
    "    n_fft=1024,\n",
    "    hop_length=512\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_audio(audio_segments, sample_rate=16000):\n",
    "    spectrograms = []\n",
    "    max_width = 0  # Find the maximum width of the spectrograms\n",
    "    \n",
    "    for audio in audio_segments:\n",
    "        waveform = torch.tensor(audio.get_array_of_samples()).float() / 32768.0\n",
    "        waveform = waveform.unsqueeze(0)  # Add channel dimension\n",
    "        spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
    "        spectrogram = torchaudio.transforms.AmplitudeToDB()(spectrogram)\n",
    "        spectrograms.append(spectrogram)\n",
    "        max_width = max(max_width, spectrogram.shape[-1])\n",
    "    \n",
    "    # Pad spectrograms to the same width\n",
    "    padded_spectrograms = []\n",
    "    for spectrogram in spectrograms:\n",
    "        padding = (0, max_width - spectrogram.shape[-1])  # Pad only along the time axis\n",
    "        padded_spectrogram = F.pad(spectrogram, padding, \"constant\", 0)\n",
    "        padded_spectrograms.append(padded_spectrogram)\n",
    "    \n",
    "    return torch.stack(padded_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jayesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, spectrograms, labels):\n",
    "        self.spectrograms = spectrograms\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.spectrograms[idx], self.labels[idx]\n",
    "    \n",
    "spectrograms = preprocess_audio([seg[0] for seg in audio_segments])  # List of audio segments\n",
    "labels = torch.tensor([seg[1] for seg in audio_segments])           # Corresponding labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectrograms, labels, test_size=0.25, random_state=42, stratify=labels)\n",
    "\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "# class AudioClassifier(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(AudioClassifier, self).__init__()\n",
    "        \n",
    "#         # CNN layers\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         # LSTM layers\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=128,  # Match features dimension\n",
    "#             hidden_size=256,  # Configurable\n",
    "#             num_layers=2,  # Configurable\n",
    "#             batch_first=True,  # Ensure batch is the first dimension\n",
    "#             bidirectional=True  # Enable if bidirectional LSTM is needed\n",
    "#         )\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(128 * 2, 128)  # *2 because bidirectional\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,  # Match features dimension\n",
    "            hidden_size=256,  # Configurable\n",
    "            num_layers=2,  # Configurable\n",
    "            batch_first=True,  # Ensure batch is the first dimension\n",
    "            bidirectional=True  # Enable if bidirectional LSTM is needed\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 2, 128)  # *2 because bidirectional LSTM (512 -> 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f\"Initial input shape: {x.shape}\")  # [batch_size, 1, freq, time]\n",
    "\n",
    "        batch_size, channels, freq, time = x.shape\n",
    "\n",
    "        # Permute to [batch_size, time, channels, freq]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        # print(f\"Shape after permute: {x.shape}\")  # [batch_size, time, channels, freq]\n",
    "\n",
    "        # Reshape to [batch_size, time, features]\n",
    "        x = x.reshape(batch_size, time, -1)  # -1 computes channels * freq automatically\n",
    "        # print(f\"Shape after reshape: {x.shape}\")  # [batch_size, time, features]\n",
    "\n",
    "        # Pass through LSTM\n",
    "        x, _ = self.lstm(x)  # [batch_size, time, hidden_size * (1 or 2)]\n",
    "        # print(f\"Shape after LSTM: {x.shape}\")\n",
    "\n",
    "        # Take the last time step\n",
    "        x = x[:, -1, :]  # [batch_size, hidden_size * (1 or 2)]\n",
    "        # print(f\"Shape after selecting last time step: {x.shape}\")\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc1(x)  # [batch_size, 128]\n",
    "        # print(f\"Shape after fc1: {x.shape}\")\n",
    "\n",
    "        x = self.fc2(x)  # [batch_size, num_classes]\n",
    "        # print(f\"Shape after fc2: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.2866\n",
      "Testing Loss: 0.4169, Accuracy: 87.28%\n",
      "Epoch 2, Training Loss: 0.2318\n",
      "Testing Loss: 0.2724, Accuracy: 89.47%\n",
      "Epoch 3, Training Loss: 0.2190\n",
      "Testing Loss: 0.2113, Accuracy: 91.03%\n",
      "Epoch 4, Training Loss: 0.2099\n",
      "Testing Loss: 0.2167, Accuracy: 91.26%\n",
      "Epoch 5, Training Loss: 0.2093\n",
      "Testing Loss: 0.2123, Accuracy: 91.43%\n",
      "Epoch 6, Training Loss: 0.2048\n",
      "Testing Loss: 0.2085, Accuracy: 91.43%\n",
      "Epoch 7, Training Loss: 0.2095\n",
      "Testing Loss: 0.2064, Accuracy: 91.26%\n",
      "Epoch 8, Training Loss: 0.2034\n",
      "Testing Loss: 0.1950, Accuracy: 92.11%\n",
      "Epoch 9, Training Loss: 0.2050\n",
      "Testing Loss: 0.2069, Accuracy: 91.16%\n",
      "Epoch 10, Training Loss: 0.1978\n",
      "Testing Loss: 0.1884, Accuracy: 92.21%\n",
      "Epoch 11, Training Loss: 0.2085\n",
      "Testing Loss: 0.3805, Accuracy: 75.69%\n",
      "Epoch 12, Training Loss: 0.2244\n",
      "Testing Loss: 0.2092, Accuracy: 91.66%\n",
      "Epoch 13, Training Loss: 0.2057\n",
      "Testing Loss: 0.1961, Accuracy: 91.99%\n",
      "Epoch 14, Training Loss: 0.1927\n",
      "Testing Loss: 0.1942, Accuracy: 92.19%\n",
      "Epoch 15, Training Loss: 0.1925\n",
      "Testing Loss: 0.2507, Accuracy: 88.41%\n",
      "Epoch 16, Training Loss: 0.2019\n",
      "Testing Loss: 0.2027, Accuracy: 91.31%\n",
      "Epoch 17, Training Loss: 0.2012\n",
      "Testing Loss: 0.1920, Accuracy: 92.21%\n",
      "Epoch 18, Training Loss: 0.2009\n",
      "Testing Loss: 0.1959, Accuracy: 92.04%\n",
      "Epoch 19, Training Loss: 0.1927\n",
      "Testing Loss: 0.1999, Accuracy: 91.84%\n",
      "Epoch 20, Training Loss: 0.1967\n",
      "Testing Loss: 0.1997, Accuracy: 91.84%\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AudioClassifier(num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for spectrograms, labels in train_loader:\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Testing Loss: {test_loss / len(test_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.84%\n",
      "[[2950   54]\n",
      " [ 270  695]]\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "model.eval()  # Set model to evaluation mode (no gradients)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for spectrograms, labels in test_loader:\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        outputs = model(spectrograms)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "conf = confusion_matrix(all_labels, all_predictions)\n",
    "print(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "# torch.save(model.state_dict(), \"audio_classifier_second.pth\")\n",
    "torch.save(model.state_dict(), \"audio_classifier_third.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8060 [00:00<?, ?it/s]c:\\Users\\Jayesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8060/8060 [08:43<00:00, 15.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# pass the audio segments through the model\n",
    "predictions = []\n",
    "for segment in tqdm(audio_segments):\n",
    "    # extract spectrogram\n",
    "    spectrogram = preprocess_audio([segment[0]])\n",
    "    spectrogram = spectrogram.to(device)\n",
    "    output = model(spectrogram)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predictions.append(predicted.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6083\n",
       "1    1977\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count 0s and 1s \n",
    "pd.Series(predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count original 0s and 1s\n",
    "count = 0\n",
    "for segment in audio_segments:\n",
    "    if segment[1] == 1:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3861"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through predictions where every 3 seconds is a prediction and there is a 1 second overlap so count accordingly\n",
    "time_drinking = 0\n",
    "current_time_drinking = 0\n",
    "in_drinking_event = False\n",
    "for i in predictions:\n",
    "    if i == 1:    \n",
    "        if not in_drinking_event:\n",
    "            current_time_drinking += 2\n",
    "            in_drinking_event = True\n",
    "        elif in_drinking_event:\n",
    "            time_drinking += 1\n",
    "    else:\n",
    "        in_drinking_event = False\n",
    "        time_drinking += current_time_drinking\n",
    "        # if current_time_drinking > 2:\n",
    "        current_time_drinking = 0\n",
    "        # else:\n",
    "        #     current_time_drinking = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2507"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_drinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
