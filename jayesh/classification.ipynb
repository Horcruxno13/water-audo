{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "result = os.environ['PATH']\n",
    "os.environ['PATH'] += os.pathsep + 'C:\\\\Program Files\\\\ffmpeg-2024-09-22-git-a577d313b2-full_build\\\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio files \n",
    "path_to_jayesh_drinking = \"DrinkingAudio\\\\JayeshDrinking\\\\\"\n",
    "path_to_jayesh_not_drinking = \"DrinkingAudio\\\\JayeshDrinking\\\\SilentRecording\\\\\"\n",
    "path_to_elvis_drinking = \"DrinkingAudio\\\\ElvisDrinking\\\\\"\n",
    "path_to_elvis_not_drinking = \"DrinkingAudio\\\\ElvisDrinking\\\\SilentRecording\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jayesh m4a files \n",
    "jayesh_drinking_files = [f for f in os.listdir(path_to_jayesh_drinking) if f.endswith('.m4a')]\n",
    "jayesh_not_drinking_files = [f for f in os.listdir(path_to_jayesh_not_drinking) if f.endswith('.m4a')]\n",
    "\n",
    "# load elvis mp4 files\n",
    "elvis_drinking_files = [f for f in os.listdir(path_to_elvis_drinking) if f.endswith('.mp4')]\n",
    "elvis_not_drinking_files = [f for f in os.listdir(path_to_elvis_not_drinking) if f.endswith('.mp4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to load audio files using pydub\n",
    "def load_audio_file(file_path):\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    audio = audio.set_frame_rate(16000)\n",
    "    audio = audio.set_channels(1)\n",
    "    return audio\n",
    "\n",
    "# load audio files\n",
    "jayesh_drinking_waveforms = [load_audio_file(path_to_jayesh_drinking + f)[0] for f in jayesh_drinking_files]\n",
    "jayesh_not_drinking_waveforms = [load_audio_file(path_to_jayesh_not_drinking + f)[0] for f in jayesh_not_drinking_files]\n",
    "elvis_drinking_waveforms = [load_audio_file(path_to_elvis_drinking + f)[0] for f in elvis_drinking_files]\n",
    "elvis_not_drinking_waveforms = [load_audio_file(path_to_elvis_not_drinking + f)[0] for f in elvis_not_drinking_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jayesh Drinking:  11\n",
      "Jayesh Not Drinking:  26\n",
      "Elvis Drinking:  16\n",
      "Elvis Not Drinking:  75\n"
     ]
    }
   ],
   "source": [
    "# count number of samples\\\n",
    "print(\"Jayesh Drinking: \", len(jayesh_drinking_waveforms))\n",
    "print(\"Jayesh Not Drinking: \", len(jayesh_not_drinking_waveforms))\n",
    "print(\"Elvis Drinking: \", len(elvis_drinking_waveforms))\n",
    "print(\"Elvis Not Drinking: \", len(elvis_not_drinking_waveforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Drinking to Not Drinking Ratio:  0.26732673267326734\n"
     ]
    }
   ],
   "source": [
    "# get total ratio of drinking to not drinking\n",
    "total_drinking = len(jayesh_drinking_waveforms) + len(elvis_drinking_waveforms)\n",
    "total_not_drinking = len(jayesh_not_drinking_waveforms) + len(elvis_not_drinking_waveforms)\n",
    "total_ratio = total_drinking / total_not_drinking\n",
    "print(\"Total Drinking to Not Drinking Ratio: \", total_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check channels length of audio files\n",
    "jayesh_drinking_waveforms[0].channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elvis_drinking_waveforms[0].channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation function\n",
    "def augment_audio(audio, augment_type=\"pitch_shift\"):\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "    \n",
    "    if augment_type == \"pitch_shift\":\n",
    "        # Pitch shifting\n",
    "        n_steps = random.randint(-2, 2)  # Shift pitch by up to 2 steps\n",
    "        shifted = librosa.effects.pitch_shift(samples.astype(float), sr=audio.frame_rate, n_steps=n_steps)\n",
    "        return AudioSegment(\n",
    "            shifted.astype(np.int16).tobytes(), \n",
    "            frame_rate=audio.frame_rate, \n",
    "            sample_width=audio.sample_width,\n",
    "            channels=audio.channels\n",
    "        )\n",
    "    elif augment_type == \"add_noise\":\n",
    "        # Adding noise\n",
    "        noise = np.random.normal(0, 0.02, len(samples))  # Adjust the noise level as needed\n",
    "        noisy = samples + noise\n",
    "        return AudioSegment(\n",
    "            noisy.astype(np.int16).tobytes(), \n",
    "            frame_rate=audio.frame_rate, \n",
    "            sample_width=audio.sample_width,\n",
    "            channels=audio.channels\n",
    "        )\n",
    "    elif augment_type == \"volume_adjust\":\n",
    "        # Adjusting volume\n",
    "        volume_change = random.uniform(-5, 5)  # Change volume by -5 to +5 dB\n",
    "        return audio + volume_change  # Adjust volume\n",
    "    else:\n",
    "        return audio  # No augmentation\n",
    "\n",
    "# Load audio files and apply augmentation\n",
    "def load_and_augment_audio_file(file_path):\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    audio = audio.set_frame_rate(16000)\n",
    "    audio = audio.set_channels(1)\n",
    "\n",
    "    # Apply augmentation (you can use multiple augmentations)\n",
    "    augmented_audios = [audio]\n",
    "    for _ in range(2):  # Create two augmented versions for each original\n",
    "        aug_type = random.choice([\"pitch_shift\", \"add_noise\", \"volume_adjust\"])  # Randomly choose an augmentation type\n",
    "        augmented_audio = augment_audio(audio, augment_type=aug_type)\n",
    "        augmented_audios.append(augmented_audio)\n",
    "\n",
    "    return augmented_audios\n",
    "\n",
    "# Load and augment audio files\n",
    "jayesh_drinking_waveforms = [load_and_augment_audio_file(path_to_jayesh_drinking + f) for f in jayesh_drinking_files]\n",
    "jayesh_not_drinking_waveforms = [load_and_augment_audio_file(path_to_jayesh_not_drinking + f) for f in jayesh_not_drinking_files]\n",
    "elvis_drinking_waveforms = [load_and_augment_audio_file(path_to_elvis_drinking + f) for f in elvis_drinking_files]\n",
    "elvis_not_drinking_waveforms = [load_and_augment_audio_file(path_to_elvis_not_drinking + f) for f in elvis_not_drinking_files]\n",
    "\n",
    "# Flatten the list of augmented waveforms\n",
    "jayesh_drinking_waveforms = [item for sublist in jayesh_drinking_waveforms for item in sublist]\n",
    "jayesh_not_drinking_waveforms = [item for sublist in jayesh_not_drinking_waveforms for item in sublist]\n",
    "elvis_drinking_waveforms = [item for sublist in elvis_drinking_waveforms for item in sublist]\n",
    "elvis_not_drinking_waveforms = [item for sublist in elvis_not_drinking_waveforms for item in sublist]\n",
    "\n",
    "# Now you can proceed to extract MFCCs and continue with the rest of the workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(audio, max_duration=40, sr=16000, n_mfcc=13):\n",
    "    # Convert pydub AudioSegment to numpy array of samples\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / 32768.0  # Normalize to [-1, 1]\n",
    "    \n",
    "    # If audio is too long, truncate to 40 seconds\n",
    "    max_samples = sr * max_duration\n",
    "    if len(samples) > max_samples:\n",
    "        samples = samples[:max_samples]\n",
    "    \n",
    "    # If audio is too short, pad with zeros\n",
    "    if len(samples) < max_samples:\n",
    "        padding = np.zeros(max_samples - len(samples))\n",
    "        samples = np.concatenate([samples, padding])\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=samples, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfcc.T  # Transpose so that we have (time, n_mfcc)\n",
    "\n",
    "# Extract MFCCs for each set of waveforms\n",
    "jayesh_drinking_mfccs = [extract_mfcc(audio) for audio in jayesh_drinking_waveforms]\n",
    "jayesh_not_drinking_mfccs = [extract_mfcc(audio) for audio in jayesh_not_drinking_waveforms]\n",
    "elvis_drinking_mfccs = [extract_mfcc(audio) for audio in elvis_drinking_waveforms]\n",
    "elvis_not_drinking_mfccs = [extract_mfcc(audio) for audio in elvis_not_drinking_waveforms]\n",
    "\n",
    "# Step 4: Combine the data into one dataset (with labels: 1 for drinking, 0 for not drinking)\n",
    "# Drinking = 1, Not drinking = 0\n",
    "all_mfccs = jayesh_drinking_mfccs + jayesh_not_drinking_mfccs + elvis_drinking_mfccs + elvis_not_drinking_mfccs\n",
    "all_labels = [1] * len(jayesh_drinking_mfccs) + [0] * len(jayesh_not_drinking_mfccs) + \\\n",
    "             [1] * len(elvis_drinking_mfccs) + [0] * len(elvis_not_drinking_mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 288\n",
      "Testing set size: 96\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_mfccs, all_labels, test_size=0.25, random_state=42, stratify=all_labels)\n",
    "\n",
    "# Modify __getitem__ in AudioDataset class to reshape the MFCCs\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Add a channel dimension (for CNN input)\n",
    "        mfcc = np.expand_dims(mfcc, axis=0)  # This adds a 1-channel dimension\n",
    "        \n",
    "        return torch.tensor(mfcc, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "\n",
    "# Step 5: Create DataLoader instances for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print the sizes of the training and testing datasets\n",
    "print(f'Training set size: {len(train_dataset)}')\n",
    "print(f'Testing set size: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class AudioClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifierCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Initialize the fully connected layer size dynamically\n",
    "        self.fc1 = nn.Linear(self._get_conv_output_size(), 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # Assuming binary classification\n",
    "\n",
    "    def _get_conv_output_size(self):\n",
    "        # Create a dummy input to calculate the size after convolution layers\n",
    "        dummy_input = torch.zeros(1, 1, 1251, 13)  # Sample input shape\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(dummy_input))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        return int(torch.prod(torch.tensor(x.size()[1:])))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU activations and max pooling\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # Flatten the input dynamically\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation for output (CrossEntropyLoss will be applied later)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create the model instance\n",
    "model = AudioClassifierCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (Cross-Entropy Loss for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer (Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.9337808622254267, Accuracy: 61.111111111111114%\n",
      "Epoch [2/10], Loss: 0.36923138466146255, Accuracy: 79.51388888888889%\n",
      "Epoch [3/10], Loss: 0.2893830719921324, Accuracy: 83.68055555555556%\n",
      "Epoch [4/10], Loss: 0.2287169810798433, Accuracy: 88.19444444444444%\n",
      "Epoch [5/10], Loss: 0.18745451834466723, Accuracy: 91.31944444444444%\n",
      "Epoch [6/10], Loss: 0.16153829875919554, Accuracy: 91.66666666666667%\n",
      "Epoch [7/10], Loss: 0.11218020733859804, Accuracy: 96.52777777777777%\n",
      "Epoch [8/10], Loss: 0.08734425115916464, Accuracy: 96.875%\n",
      "Epoch [9/10], Loss: 0.07068691526850064, Accuracy: 98.26388888888889%\n",
      "Epoch [10/10], Loss: 0.05250347860985332, Accuracy: 98.95833333333333%\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs and batch size\n",
    "num_epochs = 10\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Print epoch stats\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}, Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.83333333333333%\n",
      "Confusion Matrix:\n",
      "[[72  4]\n",
      " [ 0 20]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store true and predicted labels\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "model.eval()  # Set model to evaluation mode (no gradients)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Accumulate total correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and true labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Compute test accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the 4 false positives\n",
    "false_positives = []\n",
    "for i in range(len(all_labels)):\n",
    "    if all_labels[i] == 0 and all_predictions[i] == 1:\n",
    "        false_positives.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives:  [32, 55, 56, 79]\n"
     ]
    }
   ],
   "source": [
    "print(\"False Positives: \", false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
